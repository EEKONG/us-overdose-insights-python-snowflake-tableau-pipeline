{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f57e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73214d34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629011d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Accidental_Drug_Related_Deaths_2012-2024.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb83bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create a copy of the data before cleaning \n",
    "df_cld = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1746120",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standarding the column names\n",
    "print(df_cld.columns) # see what columns we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b2d2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns have Upper and lower case letters with spaces for some with two labels\n",
    "#lets standardize the headers to this all lower case and to this format \"x_y\"\n",
    "\n",
    "df_cld.columns = (\n",
    "     df_cld.columns.str.strip()                               #remove leading trailing/spaces\n",
    "        .str.lower()                                          #convert column names to lowercase\n",
    "        .str.replace(\" \", \"_\")                                #replace column name format \"x Y\" with \"X_Y\"\n",
    "        .str.replace(r\"[^\\w\\s]\", \"\", regex=True)              #Remove any characters that are not letters, numbers, underscores, or spaces.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1216b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cld.head(2)) # confirm that the column standardization worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some columns have all uppercase data for example df_cld[\"residence_city\"] \n",
    "#has \"BRIDGEPORT\", \"WATERBURY\", \"NORWICH\", \"BROOKFIELD\", \"NEW HAVEN\"\n",
    "#Lets standardize this\n",
    "\n",
    "col_to_standardize = [\"residence_city\", \"residence_county\", \"injury_city\", \"injury_county\"]\n",
    "\n",
    "df_cld[col_to_standardize] = df_cld[col_to_standardize].apply(lambda x: x.str.strip().str.title()) # remove leading/trailing spaces and capitalize first letter of each word\n",
    "\n",
    "print(df_cld.head(2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb3df07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets extract the latitude and longitude from the residencecitygeo, injurycitygeo and deathcitygeo\n",
    "df_cld[[\"rc_latitude\", \"rc_longitude\"]] = df_cld[\"residencecitygeo\"].str.extract(r\"\\((.*?),\\s(.*?)\\)\")\n",
    "\n",
    "#covert to float\n",
    "df_cld[\"rc_latitude\"] = df_cld[\"rc_latitude\"].astype(float)\n",
    "df_cld[\"rc_longitude\"] = df_cld[\"rc_longitude\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e48f63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets do same for the injurycitygeo\n",
    "df_cld[[\"ic_latitude\", \"ic_longitude\"]] = df_cld[\"injurycitygeo\"].str.extract(r\"\\((.*?),\\s(.*?)\\)\")\n",
    "\n",
    "df_cld[\"ic_latitude\"] = df_cld[\"ic_latitude\"].astype(float)\n",
    "df_cld[\"ic_longitude\"] = df_cld[\"ic_longitude\"].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5802b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets do same for the deathcitygeo\n",
    "df_cld[[\"dc_latitude\", \"dc_longitude\"]] = df_cld[\"deathcitygeo\"].str.extract(r\"\\((.*?),\\s(.*?)\\)\")\n",
    "\n",
    "df_cld[\"dc_latitude\"] = df_cld[\"dc_latitude\"].astype(float)\n",
    "df_cld[\"dc_longitude\"] = df_cld[\"dc_longitude\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9e72ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The state column has some null values. However, if you check the deatchcitygeo column, you would see the corresponding state\n",
    "# The injurycitygeo and residencecitygeo also have the states in them\n",
    "# We are going to extract the state values from the death city column and put it in the corresponding null state column field\n",
    "\n",
    "df_cld[\"residence_state\"] = df_cld[\"residence_state\"].fillna(df_cld[\"deathcitygeo\"].str.extract(r\"(?:^|,)\\s*([A-Z]{2})\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5302a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirming result\n",
    "print(df_cld[\"residence_state\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3854476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Fill missing values in residence_city, residence_county \"Unknown\" \n",
    "#as there is no correpsonding data for them in the geo columns\n",
    "#and they have valuable data we would need for our analysis\n",
    "df_cld[\"residence_city\"].fillna(\"Unknown\", inplace=True)\n",
    "df_cld[\"residence_county\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "\n",
    "print(df_cld[\"residence_city\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326eb50e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b7169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets standardize the race column\n",
    "print(df_cld[\"race\"].unique())\n",
    "\n",
    "# Convert to lowercase and strip spaces\n",
    "df_cld[\"race\"] = df_cld[\"race\"].str.lower().str.strip()\n",
    "\n",
    "\n",
    "\n",
    "# Map races using substring matching\n",
    "# we are going to use the official USA race classification data for this mapping\n",
    "# White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or Other Pacific Islander\n",
    "\n",
    "df_cld.loc[df_cld[\"race\"].str.contains(\"black\", na=False), \"race\"] = \"Black or African American\"\n",
    "df_cld.loc[df_cld[\"race\"].str.contains(\"white\", na=False), \"race\"] = \"White\"\n",
    "df_cld.loc[df_cld[\"race\"].str.contains(\"asian\", na=False), \"race\"] = \"Asian\"\n",
    "df_cld.loc[df_cld[\"race\"].str.contains(\"american indian|native american|lenni lenape\", na=False), \"race\"] = \"American Indian\"\n",
    "df_cld.loc[df_cld[\"race\"].str.contains(\"haitian\", na=False), \"race\"] = \"Black or African American\"\n",
    "df_cld.loc[df_cld[\"race\"].str.contains(\"portugese\", na=False), \"race\"] = \"White\"\n",
    "df_cld.loc[df_cld[\"race\"].str.contains(\"puerto rican\", na=False), \"race\"] = \"Other\"\n",
    "df_cld.loc[df_cld[\"race\"].str.contains(\"hawaiian\", na=False), \"race\"] = \"Native Hawaiian\"\n",
    "df_cld.loc[df_cld[\"race\"].str.contains(\"chinese\", na=False), \"race\"] = \"Asian\"\n",
    "df_cld.loc[df_cld[\"race\"].str.contains(\"korean\", na=False), \"race\"] = \"Asian\"\n",
    "df_cld.loc[df_cld[\"race\"].str.contains(\"japanese\", na=False), \"race\"] = \"Asian\"\n",
    "\n",
    "# Check unique values\n",
    "#print(df_cld[\"race\"].unique())\n",
    "\n",
    "#one row had Other (Specify)\n",
    "df_cld[\"race\"] = df_cld[\"race\"].str.strip().replace({\"Other (Specify)\": \"Other\"})\n",
    "\n",
    "# Fill remaining empty fields with unknown\n",
    "df_cld[\"race\"] = df_cld[\"race\"].fillna(\"Unknown\").str.title()\n",
    "\n",
    "# Check unique values\n",
    "print(df_cld[\"race\"].unique())\n",
    "\n",
    "#now we have a clean output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451f8cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39091bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets clean up the ethincity column\n",
    "\n",
    "df_cld[\"ethnicity\"] = df[\"Ethnicity\"]\n",
    "#print(df_cld[\"ethnicity\"].unique())\n",
    "\n",
    "df_cld[\"ethnicity\"] = df_cld[\"ethnicity\"].fillna(\"Unknown\").str.strip().str.title()\n",
    "\n",
    "#The ethnicity column has multiple variations of the hispanic and non-hispanic\n",
    "#That would be our standardized values for this column \"Hispanic\" and \"Non-Hispanic\"\n",
    "\n",
    "df_cld.loc[\n",
    "    df_cld[\"ethnicity\"].str.match(\n",
    "        r\"^No, Not Spanish/Hispanic/Latino$|^Not Spanish/Hispanic/Latino$|^n$\", \n",
    "        case=False, na=False\n",
    "    ),\n",
    "    \"ethnicity\"\n",
    "] = \"Non-Hispanic\"\n",
    "\n",
    "df_cld.loc[\n",
    "    (~df_cld[\"ethnicity\"].isin([\"Non-Hispanic\", \"Unknown\"])) &\n",
    "    (df_cld[\"ethnicity\"].str.contains(\n",
    "        r\"Yes|Hispanic|Latino|Puerto Rican|Mexican|Cuban|Other Spanish\", \n",
    "        case=False, na=False\n",
    "    )),\n",
    "    \"ethnicity\"\n",
    "] = \"Hispanic\"\n",
    "\n",
    "\n",
    "print(df_cld[\"ethnicity\"].unique()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e081a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drugs' columns have Y and NaN values in them\n",
    "\n",
    "#print(df_cld.columns)\n",
    "\n",
    "#For forecaseting purposes we will change the Y to 1 and NaN values to 0\n",
    "\n",
    "drug_columns = [\n",
    "    'heroin', 'cocaine', 'fentanyl', 'fentanyl_analogue', 'oxycodone',\n",
    "    'oxymorphone', 'ethanol', 'hydrocodone', 'benzodiazepine', 'methadone',\n",
    "    'methamphetamine', 'amphet', 'tramad', 'hydromorphone',\n",
    "    'morphine_not_heroin', 'xylazine', 'gabapentin', 'opiate_nos',\n",
    "    'heroinmorphcodeine', 'any_opioid'\n",
    "]\n",
    "\n",
    "df_cld[drug_columns] = df_cld[drug_columns].replace({\n",
    "    \"Y\": 1, \"y\": 1, \n",
    "    \"Y POPS\": 1,  \n",
    "    \"N\": 0, \n",
    "    \"n\": 0, \n",
    "    \"P\":np.nan, \n",
    "    \"Y (PTCH)\": 1, \n",
    "    \"NO RX BUT STRAWS\": 0, \n",
    "    \"STOLE MEDS\": 0, \n",
    "    \"PCP NEG\":0, \n",
    "    \"N\":0}).fillna(0)\n",
    "\n",
    "#Converting to integer\n",
    "# df_cld[drug_columns].astype('int8')\n",
    "\n",
    "#got an error - ValueError: invalid literal for int() with base 10: 'Y POPS'\n",
    "#Seems some drug columns have non numeric values\n",
    "#lets fish them out\n",
    "for col in drug_columns:\n",
    "    bad_values = df_cld[~df_cld[col].isin(['Y', 'N', 'P', np.nan])][col].unique()\n",
    "    if len(bad_values) > 0:\n",
    "        print(f\"{col}: {bad_values}\")\n",
    "\n",
    "\n",
    "#found them - fentanyl: [1 0 'Y POPS' 'Y (PTCH)'] and morphine_not_heroin: [0 1 'NO RX BUT STRAWS' 'STOLE MEDS' 'PCP NEG']\n",
    "\n",
    "#The former likely means yes, converting to --> 1. The latter likely means No, converting to --> 0\n",
    "#adding them to the initial replace code above\n",
    "\n",
    "df_cld[drug_columns] = df_cld[drug_columns].astype('int8')\n",
    "\n",
    "#Lets see if that worked\n",
    "print(df_cld[drug_columns].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95642447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeing what data type our drug columns are\n",
    "print(df_cld[drug_columns].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d187dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets clean up the location column\n",
    "\n",
    "#print(df_cld[\"location\"].unique()) - -Checking what values are in the column\n",
    "\n",
    "#location mapping\n",
    "locatn_map = {\n",
    "    \"Decedent’s Home\": \"Home\",\n",
    "    \"Decedent's Home\": \"Home\",\n",
    "    \"Residence\": \"Home\",\n",
    "    \"Hospital\": \"Hospital\",\n",
    "    \"Hiospital\": \"Hospital\",\n",
    "    \"Hospital - ER/Outpatient\": \"Hospital\",\n",
    "    \"Hospital - Inpatient\": \"Hospital\",\n",
    "    \"Hospital - Dead On Arrival\": \"Hospital\",\n",
    "    \"Hospice\": \"Hospice\",\n",
    "    \"Hospice Facility\": \"Hospice\",\n",
    "    \"Nursing Home\": \"Care Facility\",\n",
    "    \"Convalescent Home\": \"Care Facility\",\n",
    "    \"Assisted Living\": \"Care Facility\",\n",
    "    \"Shelter\": \"Shelter\",\n",
    "    \"Other\": \"Other\",\n",
    "    \"Other (Specify)\": \"Other\"\n",
    "}\n",
    "\n",
    "df_cld[\"location\"] = df_cld[\"location\"].str.strip().replace(locatn_map).fillna(\"Unknown\")\n",
    "\n",
    "print(df_cld[\"location\"].unique()) #cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de024a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert age column to integer\n",
    "df_cld[\"age\"] = df_cld[\"age\"].astype('Int64')\n",
    "\n",
    "print(df_cld[\"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b640a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df_cld[\"sex\"].isnull().sum()) #seeing 9 null values in the sex column\n",
    "\n",
    "#lets convert the null values to Unknown as they have data we need\n",
    "df_cld[\"sex\"] = df_cld[\"sex\"].fillna(\"Unknown\")\n",
    "\n",
    "print(df_cld[\"sex\"].unique()) #That worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6885c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0281b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb584d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a0b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doublechecking how the dataframe currently looks\n",
    "print(df_cld.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e236d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a01fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7f04c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets change the date column into a datetime column\n",
    "\n",
    "df_cld[\"date\"] = pd.to_datetime(df_cld[\"date\"])\n",
    "\n",
    "print(df_cld.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ed1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6125be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the manner of death column\n",
    "\n",
    "print(df_cld[\"manner_of_death\"].unique())\n",
    "\n",
    "df_cld[\"manner_of_death\"] = df_cld[\"manner_of_death\"].str.strip().replace({\n",
    "    \"accident\": \"Accident\",\n",
    "    \"Acciddent\": \"Accident\",\n",
    "    \"ACCIDENT\": \"Accident\",\n",
    "}).fillna(\"Unknown\")\n",
    "\n",
    "print(df_cld[\"manner_of_death\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934479bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Lets drop columns we do not need\n",
    "\n",
    "#lets create a copy first\n",
    "df_cleaned = df_cld.copy()\n",
    "\n",
    "print(df_cleaned.columns)\n",
    "\n",
    "\n",
    "df_cleaned.drop([\n",
    "    \"location_if_other\",\n",
    "    \"heroin_death_certificate_dc\",\n",
    "    \"other\", \"other_opioid\",\n",
    "    \"description_of_injury\",\n",
    "    \"death_state\",\n",
    "    \"death_county\",\n",
    "    \"residencecitygeo\", \n",
    "    \"injurycitygeo\", \n",
    "    \"deathcitygeo\",\n",
    "    \"injury_place\",\n",
    "    \"other_significant_conditions\"\n",
    "], axis = 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d97783",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ddb760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e9cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned[\"death_city\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21194334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets categorize the cause_of_death column as it has values like \"Acute Intoxication by the Combined Effects of Fentanyl and Cocaine\"\n",
    "\n",
    "#Lets create a function we would apply to that column\n",
    "def categorize_cause_of_death(cause):\n",
    "    categories = []\n",
    "    if 'Fentanyl' in cause:\n",
    "        categories.append('Fentanyl')\n",
    "    if 'Cocaine' in cause:\n",
    "        categories.append('Cocaine')\n",
    "    if 'Heroin' in cause:\n",
    "        categories.append('Heroin')\n",
    "    if 'Ethanol' in cause:\n",
    "        categories.append('Ethanol')\n",
    "    if not categories:\n",
    "        return 'Other'\n",
    "    return '+'.join(categories) + '-related'\n",
    "\n",
    "\n",
    "df_cleaned['cause_of_death'] = df_cleaned['cause_of_death'].apply(categorize_cause_of_death)\n",
    "\n",
    "print(df_cleaned[\"cause_of_death\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f81139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets clean up the death city column\n",
    "\n",
    "df_cleaned[\"death_city\"] = df_cleaned[\"death_city\"].str.strip().str.title().replace({\n",
    "    \"6430\": \"Groton\"\n",
    "}).fillna(\"Unknown\")\n",
    "\n",
    "print(df_cleaned[\"death_city\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3fd23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b520ea2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the injury county column\n",
    "\n",
    "df_cleaned[\"injury_county\"] = df_cleaned[\"injury_county\"].str.strip().str.title().replace({\"Mnew London\": \"New London\"}).fillna(\"Unknown\")\n",
    "\n",
    "print(df_cleaned[\"injury_county\"].unique())\n",
    "\n",
    "df_cleaned[\"injury_state\"] = df_cleaned[\"injury_state\"].str.strip().str.upper().fillna(\"Unknown\")\n",
    "\n",
    "df_cleaned[\"injury_state\"]= df_cleaned[\"injury_state\"].replace({\"MASSACHUSSETS\": \"MA\", \"CONNECTICUT\": \"CT\", \"UKNOWN\": \"UNKNOWN\"})\n",
    "print(df_cleaned[\"injury_state\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc4edde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f0017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned[\"death_city\"].unique())\n",
    "\n",
    "df_cleaned_backup = df_cleaned.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned[\"race\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf60a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48796c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lETS DO SOME FORECASTING!!\n",
    "\n",
    "#I will be using Prophet for thie forecast\n",
    "#!pip install prophet cmdstanpy - Installing prophet\n",
    "\n",
    "# 1. Forecasting total overdose deaths over time using prophet\n",
    "\n",
    "#lets find the total deaths per month\n",
    "\n",
    "death_per_month = (\n",
    "    df_cleaned.groupby(df_cleaned[\"date\"].dt.to_period(\"M\"))\n",
    "    .size()\n",
    "    .reset_index(name=\"death_count\")\n",
    ")\n",
    "\n",
    "death_per_month[\"date\"] = death_per_month[\"date\"].dt.to_timestamp()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b2d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirming the data is accurate\n",
    "print(death_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736372b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546cc1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dcc677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "df_cleaned_forecast =  death_per_month.rename(columns={\"date\": \"ds\", \"death_count\":\"y\"})\n",
    "\n",
    "\n",
    "model = Prophet()\n",
    "model.fit(df_cleaned_forecast)\n",
    "\n",
    "#Predict next 12 months\n",
    "future = model.make_future_dataframe(periods=12, freq=\"M\")\n",
    "forecast = model.predict(future)\n",
    "\n",
    "\n",
    "#lets use only what we need\n",
    "forecasted_monthly_death = forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]]\n",
    "forecasted_monthly_death.columns = [\"date\", \"forecast\", \"lower_bound\", \"upper_bound\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d89e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forecasted_monthly_death.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e436fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. lets see yearly forecasted trends for two prevalent drugs - fentanyly and cocaine\n",
    "\n",
    "#lets first create a year column\n",
    "df_cleaned[\"year\"] = df_cleaned[\"date\"].dt.year\n",
    "\n",
    "yearly_trends = (\n",
    "    df_cleaned.groupby(\"year\")[[\"fentanyl\", \"cocaine\"]]\n",
    "    .apply(lambda x: pd.Series({\n",
    "        \"fentanyl_cases\": (x[\"fentanyl\"] == 1).sum(),\n",
    "        \"cocaine_cases\": (x[\"cocaine\"] == 1).sum()\n",
    "    })\n",
    ").reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "print(yearly_trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3febf2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "from prophet import Prophet\n",
    "\n",
    "\n",
    "# Fentanyl Forecast\n",
    "fentanyl_df = yearly_trends.rename(columns={\"year\":\"ds\", \"fentanyl_cases\":\"y\"})\n",
    "fentanyl_df[\"ds\"] = pd.to_datetime(fentanyl_df[\"ds\"], format=\"%Y\")\n",
    "\n",
    "# Log-transform y to handle large range\n",
    "fentanyl_df[\"y\"] = np.log1p(fentanyl_df[\"y\"])\n",
    "\n",
    "model_fent = Prophet(\n",
    "    growth=\"linear\",\n",
    "    changepoint_prior_scale=0.3,  # flexible to capture surges\n",
    "    yearly_seasonality=False\n",
    ")\n",
    "model_fent.fit(fentanyl_df)\n",
    "\n",
    "future_fent = model_fent.make_future_dataframe(periods=3, freq=\"Y\")\n",
    "forecast_fent = model_fent.predict(future_fent)\n",
    "\n",
    "# Convert back from log-transform\n",
    "forecast_fent[\"yhat\"] = np.expm1(forecast_fent[\"yhat\"])\n",
    "forecast_fent[\"yhat_lower\"] = np.expm1(forecast_fent[\"yhat_lower\"])\n",
    "forecast_fent[\"yhat_upper\"] = np.expm1(forecast_fent[\"yhat_upper\"])\n",
    "\n",
    "# Clip negatives to 0\n",
    "forecast_fent[\"yhat\"] = forecast_fent[\"yhat\"].clip(lower=0)\n",
    "forecast_fent[\"yhat_lower\"] = forecast_fent[\"yhat_lower\"].clip(lower=0)\n",
    "forecast_fent[\"yhat_upper\"] = forecast_fent[\"yhat_upper\"].clip(lower=0)\n",
    "\n",
    "fentanyl_forecast = forecast_fent[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]]\n",
    "fentanyl_forecast.columns = [\"year\", \"forecast\", \"lower_bound\", \"upper_bound\"]\n",
    "fentanyl_forecast[\"substance\"] = \"Fentanyl\"\n",
    "\n",
    "# now LETS FORECAST COCAINA!!!!\n",
    "cocaine_df = yearly_trends.rename(columns={\"year\": \"ds\", \"cocaine_cases\": \"y\"})\n",
    "cocaine_df[\"ds\"] = pd.to_datetime(cocaine_df[\"ds\"], format=\"%Y\")\n",
    "cocaine_df[\"y\"] = np.log1p(cocaine_df[\"y\"])\n",
    "\n",
    "model_coc = Prophet(\n",
    "    growth=\"linear\",\n",
    "    changepoint_prior_scale=0.3,\n",
    "    yearly_seasonality=False\n",
    ")\n",
    "model_coc.fit(cocaine_df)\n",
    "\n",
    "future_coc = model_coc.make_future_dataframe(periods=3, freq=\"Y\")\n",
    "forecast_coc = model_coc.predict(future_coc)\n",
    "\n",
    "forecast_coc[\"yhat\"] = np.expm1(forecast_coc[\"yhat\"])\n",
    "forecast_coc[\"yhat_lower\"] = np.expm1(forecast_coc[\"yhat_lower\"])\n",
    "forecast_coc[\"yhat_upper\"] = np.expm1(forecast_coc[\"yhat_upper\"])\n",
    "\n",
    "forecast_coc[\"yhat\"] = forecast_coc[\"yhat\"].clip(lower=0)\n",
    "forecast_coc[\"yhat_lower\"] = forecast_coc[\"yhat_lower\"].clip(lower=0)\n",
    "forecast_coc[\"yhat_upper\"] = forecast_coc[\"yhat_upper\"].clip(lower=0)\n",
    "\n",
    "cocaine_forecast = forecast_coc[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]]\n",
    "cocaine_forecast.columns = [\"year\", \"forecast\", \"lower_bound\", \"upper_bound\"]\n",
    "cocaine_forecast[\"substance\"] = \"Cocaine\"\n",
    "\n",
    "# Combining both\n",
    "yearly_forecast = pd.concat([fentanyl_forecast, cocaine_forecast], ignore_index=True)\n",
    "\n",
    "print(yearly_forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets combine our forecasted data and actual data into one dataframe\n",
    "\n",
    "fentanyl_actuals = yearly_trends[['year', 'fentanyl_cases']].copy()\n",
    "fentanyl_actuals.rename(columns={'fentanyl_cases': 'actual'}, inplace=True)\n",
    "fentanyl_actuals['substance'] = 'Fentanyl'\n",
    "\n",
    "cocaine_actuals = yearly_trends[['year', 'cocaine_cases']].copy()\n",
    "cocaine_actuals.rename(columns={'cocaine_cases': 'actual'}, inplace=True)\n",
    "cocaine_actuals['substance'] = 'Cocaine'\n",
    "\n",
    "#Combine actuals into a single DataFrame\n",
    "actuals_combined = pd.concat([fentanyl_actuals, cocaine_actuals], ignore_index=True)\n",
    "\n",
    "#ensuring the year columns match type\n",
    "actuals_combined['year'] = pd.to_datetime(actuals_combined['year'], format='%Y')\n",
    "yearly_forecast['year'] = pd.to_datetime(yearly_forecast['year'])\n",
    "\n",
    "#Merge forecast with actuals\n",
    "merged_table = pd.merge(\n",
    "    yearly_forecast,\n",
    "    actuals_combined,\n",
    "    how='left',\n",
    "    on=['year', 'substance']\n",
    ")\n",
    "\n",
    "#Lets sort dataframe\n",
    "merged_table.sort_values(by=['substance', 'year'], inplace=True)\n",
    "merged_table.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "print(merged_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3278505",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets also forceast average age of victims for next 3 years\n",
    "\n",
    "# Aggregate average age per year\n",
    "avg_age_yearly = df_cleaned.groupby(\"year\")[\"age\"].mean().reset_index()\n",
    "avg_age_yearly.rename(columns={\"age\":\"y\"}, inplace=True)\n",
    "avg_age_yearly[\"ds\"] = pd.to_datetime(avg_age_yearly[\"year\"], format=\"%Y\")\n",
    "\n",
    "# Prophet model\n",
    "model_age = Prophet(growth=\"linear\", changepoint_prior_scale=0.3, yearly_seasonality=False)\n",
    "model_age.fit(avg_age_yearly[[\"ds\",\"y\"]])\n",
    "\n",
    "# Forecast next 3 years\n",
    "future_age = model_age.make_future_dataframe(periods=3, freq=\"Y\")\n",
    "forecast_age = model_age.predict(future_age)\n",
    "\n",
    "# Build DataFrame\n",
    "age_forecast = forecast_age[[\"ds\",\"yhat\",\"yhat_lower\",\"yhat_upper\"]].copy()\n",
    "age_forecast.columns = [\"year\",\"forecast\",\"lower_bound\",\"upper_bound\"]\n",
    "age_forecast[\"metric\"] = \"Average Age\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f6c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confirm the outputs\n",
    "\n",
    "print(avg_age_yearly)\n",
    "print(age_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3348663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets forecast cases by Race\n",
    "\n",
    "\n",
    "#Lets count how many deaths happened per year\n",
    "count_race_year = df_cleaned.groupby([\"year\", \"race\"]).size().reset_index(name=\"deaths\")\n",
    "\n",
    "#print(count_race_year)\n",
    "\n",
    "forecasts = []\n",
    "\n",
    "for race, group in count_race_year.groupby(\"race\"):\n",
    "    if group[\"year\"].nunique() < 2:\n",
    "        print(f\"Skipping {race} — not enough data for forecast.\")\n",
    "        continue\n",
    "\n",
    "    df_race = group.rename(columns={\"year\": \"ds\", \"deaths\": \"y\"})\n",
    "    df_race[\"ds\"] = pd.to_datetime(df_race[\"ds\"], format=\"%Y\")\n",
    "\n",
    "    model = Prophet(\n",
    "        changepoint_prior_scale=0.3,\n",
    "        yearly_seasonality=False,\n",
    "        growth=\"linear\"\n",
    "    )\n",
    "    model.fit(df_race)\n",
    "\n",
    "    future = model.make_future_dataframe(periods=3, freq=\"Y\")\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    results = forecast[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]]\n",
    "    results[\"race\"] = race\n",
    "    forecasts.append(results)\n",
    "\n",
    "# Combine everything\n",
    "race_forecasts = pd.concat(forecasts, ignore_index=True)\n",
    "race_forecasts = race_forecasts.rename(columns={\n",
    "    \"ds\": \"year\",\n",
    "    \"yhat\": \"forecast\",\n",
    "    \"yhat_lower\": \"lower_bound\",\n",
    "    \"yhat_upper\": \"upper_bound\"\n",
    "})\n",
    "\n",
    "race_forecasts[\"forecast\"] = race_forecasts[\"forecast\"].round().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c37925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d6ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc23ac35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Cleaning our data column onr final time to avoid errors or incorrect data inference when pushed to tableau\n",
    "\n",
    "\n",
    "#Lets rename columns that also key names in snowflake to avoid errors\n",
    "df_cleaned = df_cleaned.rename(columns={\n",
    "    'date': 'event_date',\n",
    "    'year': 'event_year'\n",
    "})\n",
    "\n",
    "#Lets restandardize the event_date column too avoid snowflake infering a wrong data type\n",
    "df_cleaned['event_date'] = pd.to_datetime(df_cleaned['event_date'], errors='coerce')\n",
    "df_cleaned['event_date'] = df_cleaned['event_date'].dt.tz_localize(None)\n",
    "df_cleaned['event_date'] = df_cleaned['event_date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "print(df_cleaned['event_date'].dtype)\n",
    "\n",
    "\n",
    "print(df_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe9085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets upgrade our pandas for the snowflake environment\n",
    "#!pip install --user --upgrade \"snowflake-connector-python[pandas]\" pandas numpy\n",
    "\n",
    "#!pip install snowflake-connector-python - Installed the snowflake connector\n",
    "\n",
    "#Lets push our data to our data warehouse --> snowflake for staging and production\n",
    "\n",
    "import snowflake.connector\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user='******',\n",
    "    password='**********',\n",
    "    account='*******',\n",
    "    warehouse='COMPUTE_WH',\n",
    "    database='LEARNSQL',\n",
    "    schema='PUBLIC',\n",
    "    role=\"ACCOUNTADMIN\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c2a1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets confirm if we connected to snowflake\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT CURRENT_USER(), CURRENT_ACCOUNT(), CURRENT_REGION();\")\n",
    "result = cur.fetchone()\n",
    "print(\"Connected as:\", result)\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e9def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets move our clean and forecasted data\n",
    "\n",
    "\n",
    "success_accident, chunks_accident, rows_accident, _ = write_pandas(conn, df_cleaned, \"accident_deaths_data\", 'LEARNSQL', 'PUBLIC')\n",
    "#success_forecast, chunks_forecast, rows_forecast, _ = write_pandas(conn, yearly_forecast, \"FORECAST_AD_DEATHS\", auto_create_table=True)\n",
    "\n",
    "print(f\"ACTUALS_TABLE upload: success={success_accident}, rows={rows_accident}\")\n",
    "#print(f\"FORECAST_TABLE upload: success={success_forecast}, rows={rows_forecast}\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1577ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets also create an excel file\n",
    "\n",
    "with pd.ExcelWriter(\"overdose_analysis.xlsx\") as writer:\n",
    "    df_cleaned.to_excel(writer, sheet_name=\"Raw_Cleaned_Data\", index=False)\n",
    "    merged_table.to_excel(writer, sheet_name=\"Forecasted_Data\", index=False)\n",
    "\n",
    "print(\"Excel file created: overdose_analysis.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
