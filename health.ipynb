{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995c3a27",
   "metadata": {},
   "source": [
    "# US Overdose Insights — Python → Snowflake → Tableau\n",
    "\n",
    "This notebook cleans and models the **Accidental Drug Related Deaths (2012–2024)** dataset and prepares it for a simple analytics pipeline:\n",
    "\n",
    "**Python (pandas) → Snowflake (staging table) → Tableau dashboard**\n",
    "\n",
    "## What an interviewer should look for\n",
    "- Clear, reproducible data cleaning (column standardization, geo parsing, missing-value strategy)\n",
    "- Basic quality checks to prevent silent data issues\n",
    "- Time-series aggregation and lightweight forecasting (Prophet)\n",
    "- Production-minded exports (idempotent load to Snowflake; optional, gated by environment variables)\n",
    "\n",
    "> **Note:** Snowflake credentials are **not** hard-coded. Set environment variables (see _Configuration_ below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a081bb18",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "### Data\n",
    "Place the raw CSV in `data/` (recommended for GitHub):\n",
    "\n",
    "- `data/Accidental_Drug_Related_Deaths_2012-2024.csv`\n",
    "\n",
    "### Python environment\n",
    "Recommended:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Minimum packages used here:\n",
    "- pandas, numpy\n",
    "- matplotlib\n",
    "- prophet (optional; only needed for forecasting)\n",
    "- snowflake-connector-python[pandas] (optional; only needed to load to Snowflake)\n",
    "\n",
    "### Snowflake (optional)\n",
    "Set these environment variables if you want to run the Snowflake load:\n",
    "- `SNOWFLAKE_USER`\n",
    "- `SNOWFLAKE_PASSWORD`\n",
    "- `SNOWFLAKE_ACCOUNT`\n",
    "- `SNOWFLAKE_WAREHOUSE`\n",
    "- `SNOWFLAKE_DATABASE`\n",
    "- `SNOWFLAKE_SCHEMA`\n",
    "\n",
    "If they are not set, the Snowflake section will be skipped safely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482522cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Parameters (edit here) ----\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "DATA_PATH = Path(\"data/Accidental_Drug_Related_Deaths_2012-2024.csv\")\n",
    "\n",
    "# Forecast horizon (months)\n",
    "FORECAST_PERIODS = 12\n",
    "\n",
    "# Export toggles\n",
    "EXPORT_CLEAN_CSV = True\n",
    "EXPORT_TO_SNOWFLAKE = True  # will auto-skip if env vars are missing\n",
    "\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db82194",
   "metadata": {},
   "source": [
    "## 1) Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00194a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''Return a copy with snake_case column names and trimmed whitespace.'''\n",
    "    out = df.copy()\n",
    "    out.columns = (\n",
    "        out.columns\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
    "        .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "    )\n",
    "    return out\n",
    "\n",
    "_geo_re = re.compile(r\"\\((.*?),\\s*(.*?)\\)\")\n",
    "\n",
    "def extract_lat_long(series: pd.Series) -> pd.DataFrame:\n",
    "    '''Extract lat/long from strings like '(41.7, -72.6)'. Returns two float columns.'''\n",
    "    extracted = series.astype(\"string\").str.extract(_geo_re)\n",
    "    extracted.columns = [\"latitude\", \"longitude\"]\n",
    "    return extracted.astype(\"float64\")\n",
    "\n",
    "_state_re = re.compile(r\"(?:^|,)\\s*([A-Z]{2})\\s*$\")\n",
    "\n",
    "def extract_state_from_geo(series: pd.Series) -> pd.Series:\n",
    "    '''Extract state code from geo strings like 'Hartford, CT'.'''\n",
    "    return series.astype(\"string\").str.extract(_state_re)[0]\n",
    "\n",
    "def clean_title(series: pd.Series) -> pd.Series:\n",
    "    '''Strip, title-case, and replace empty with NaN.'''\n",
    "    s = series.astype(\"string\").str.strip()\n",
    "    s = s.replace({\"\": pd.NA})\n",
    "    return s.str.title()\n",
    "\n",
    "def clean_upper(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(\"string\").str.strip()\n",
    "    s = s.replace({\"\": pd.NA})\n",
    "    return s.str.upper()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbaa3ec",
   "metadata": {},
   "source": [
    "## 2) Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365e3d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Missing {DATA_PATH}. Put the CSV under ./data/ (see the Configuration section).\"\n",
    "    )\n",
    "\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "df_raw.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240e0c9e",
   "metadata": {},
   "source": [
    "### Quick sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c17a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_raw.head(3))\n",
    "display(df_raw.tail(3))\n",
    "\n",
    "# Missingness overview (top 15)\n",
    "missing = (df_raw.isna().mean().sort_values(ascending=False).head(15) * 100).round(1)\n",
    "missing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ab870",
   "metadata": {},
   "source": [
    "## 3) Clean & standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e480d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = standardize_columns(df_raw)\n",
    "\n",
    "# Example: many city/county fields are inconsistent in casing; normalize key text columns\n",
    "text_title_cols = [\n",
    "    \"residence_city\", \"residence_county\", \"injury_city\", \"injury_county\", \"death_city\", \"death_county\"\n",
    "]\n",
    "for col in [c for c in text_title_cols if c in df.columns]:\n",
    "    df[col] = clean_title(df[col])\n",
    "\n",
    "# Normalize state columns to uppercase 2-letter codes when possible\n",
    "state_cols = [\"residence_state\", \"injury_state\", \"death_state\"]\n",
    "for col in [c for c in state_cols if c in df.columns]:\n",
    "    df[col] = clean_upper(df[col])\n",
    "\n",
    "# Parse geo fields to numeric lat/long (if present)\n",
    "geo_map = {\n",
    "    \"residencecitygeo\": (\"rc_latitude\", \"rc_longitude\"),\n",
    "    \"injurycitygeo\": (\"ic_latitude\", \"ic_longitude\"),\n",
    "    \"deathcitygeo\": (\"dc_latitude\", \"dc_longitude\"),\n",
    "}\n",
    "for geo_col, (lat_col, lon_col) in geo_map.items():\n",
    "    if geo_col in df.columns:\n",
    "        coords = extract_lat_long(df[geo_col])\n",
    "        df[lat_col] = coords[\"latitude\"]\n",
    "        df[lon_col] = coords[\"longitude\"]\n",
    "\n",
    "# Fill missing state using geo strings (most reliable among available fields)\n",
    "if \"residence_state\" in df.columns and \"deathcitygeo\" in df.columns:\n",
    "    df[\"residence_state\"] = df[\"residence_state\"].fillna(extract_state_from_geo(df[\"deathcitygeo\"]))\n",
    "\n",
    "# Common 'Unknown' fills for location dimensions\n",
    "for col in [\"residence_city\", \"residence_county\", \"injury_city\", \"injury_county\", \"death_city\", \"death_county\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(\"Unknown\")\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c9cc06",
   "metadata": {},
   "source": [
    "### Dates & key fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028bc929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse date and derive year/month for analysis\n",
    "if \"date\" in df.columns:\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "\n",
    "df[\"year\"] = df[\"date\"].dt.year\n",
    "df[\"month\"] = df[\"date\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "\n",
    "# Basic integrity checks\n",
    "assert df[\"date\"].notna().mean() > 0.95, \"Too many missing/invalid dates after parsing.\"\n",
    "df[[\"date\",\"year\",\"month\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610aec0",
   "metadata": {},
   "source": [
    "### Drug indicator columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many datasets encode drug presence as Y/N or 0/1. Standardize to 0/1 ints when present.\n",
    "drug_columns = [c for c in df.columns if c.startswith(\"heroin\") or c.startswith(\"cocaine\") or c.startswith(\"fentanyl\") or c.startswith(\"benzodiazepine\") or c.startswith(\"alcohol\")]\n",
    "# If your dataset uses different names, extend this list.\n",
    "\n",
    "def normalize_yes_no_to_int(s: pd.Series) -> pd.Series:\n",
    "    s2 = s.astype(\"string\").str.strip().str.upper()\n",
    "    return s2.map({\"Y\": 1, \"YES\": 1, \"TRUE\": 1, \"1\": 1, \"N\": 0, \"NO\": 0, \"FALSE\": 0, \"0\": 0})\n",
    "\n",
    "for col in drug_columns:\n",
    "    df[col] = normalize_yes_no_to_int(df[col]).astype(\"Int64\")\n",
    "\n",
    "drug_columns[:10], len(drug_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49e6cf7",
   "metadata": {},
   "source": [
    "## 4) Analytics tables (for Tableau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62edf10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deaths per month\n",
    "death_per_month = (\n",
    "    df.groupby(\"month\", dropna=False)\n",
    "      .size()\n",
    "      .reset_index(name=\"death_count\")\n",
    "      .sort_values(\"month\")\n",
    ")\n",
    "\n",
    "display(death_per_month.tail(6))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(death_per_month[\"month\"], death_per_month[\"death_count\"])\n",
    "plt.title(\"Accidental drug-related deaths per month\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Example: deaths by race (if present)\n",
    "if \"race\" in df.columns:\n",
    "    deaths_by_race = df.groupby(\"race\").size().reset_index(name=\"death_count\").sort_values(\"death_count\", ascending=False)\n",
    "    display(deaths_by_race.head(10))\n",
    "else:\n",
    "    deaths_by_race = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7538ca68",
   "metadata": {},
   "source": [
    "## 5) Forecasting (Prophet, optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5e365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet is optional; if unavailable, we skip forecasting.\n",
    "try:\n",
    "    from prophet import Prophet  # type: ignore\n",
    "    has_prophet = True\n",
    "except Exception as e:\n",
    "    has_prophet = False\n",
    "    print(\"Prophet not installed or failed to import. Skipping forecasting. Details:\", e)\n",
    "\n",
    "forecasted_monthly_death = None\n",
    "\n",
    "if has_prophet:\n",
    "    df_prophet = death_per_month.rename(columns={\"month\": \"ds\", \"death_count\": \"y\"}).dropna()\n",
    "    model = Prophet()\n",
    "    model.fit(df_prophet)\n",
    "\n",
    "    future = model.make_future_dataframe(periods=FORECAST_PERIODS, freq=\"M\")\n",
    "    fc = model.predict(future)\n",
    "\n",
    "    forecasted_monthly_death = fc[[\"ds\", \"yhat\", \"yhat_lower\", \"yhat_upper\"]].copy()\n",
    "    forecasted_monthly_death.columns = [\"month\", \"forecast\", \"lower_bound\", \"upper_bound\"]\n",
    "    display(forecasted_monthly_death.tail(6))\n",
    "\n",
    "    # Plot\n",
    "    plt.figure()\n",
    "    plt.plot(death_per_month[\"month\"], death_per_month[\"death_count\"], label=\"actual\")\n",
    "    plt.plot(forecasted_monthly_death[\"month\"], forecasted_monthly_death[\"forecast\"], label=\"forecast\")\n",
    "    plt.title(\"Monthly deaths — actual vs forecast\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Deaths\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0464c1dd",
   "metadata": {},
   "source": [
    "## 6) Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eec185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset export (for Tableau local testing / sharing)\n",
    "clean_csv_path = OUTPUT_DIR / \"accidental_drug_related_deaths_clean.csv\"\n",
    "if EXPORT_CLEAN_CSV:\n",
    "    df.to_csv(clean_csv_path, index=False)\n",
    "    print(\"Wrote:\", clean_csv_path)\n",
    "\n",
    "# Optional: Snowflake load (skips if env vars missing)\n",
    "def snowflake_env_ready() -> bool:\n",
    "    required = [\n",
    "        \"SNOWFLAKE_USER\", \"SNOWFLAKE_PASSWORD\", \"SNOWFLAKE_ACCOUNT\",\n",
    "        \"SNOWFLAKE_WAREHOUSE\", \"SNOWFLAKE_DATABASE\", \"SNOWFLAKE_SCHEMA\",\n",
    "    ]\n",
    "    return all(os.getenv(k) for k in required)\n",
    "\n",
    "if EXPORT_TO_SNOWFLAKE and snowflake_env_ready():\n",
    "    import snowflake.connector\n",
    "    from snowflake.connector.pandas_tools import write_pandas\n",
    "\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=os.environ[\"SNOWFLAKE_USER\"],\n",
    "        password=os.environ[\"SNOWFLAKE_PASSWORD\"],\n",
    "        account=os.environ[\"SNOWFLAKE_ACCOUNT\"],\n",
    "        warehouse=os.environ[\"SNOWFLAKE_WAREHOUSE\"],\n",
    "        database=os.environ[\"SNOWFLAKE_DATABASE\"],\n",
    "        schema=os.environ[\"SNOWFLAKE_SCHEMA\"],\n",
    "    )\n",
    "\n",
    "    # Load cleaned table\n",
    "    success, chunks, rows, _ = write_pandas(\n",
    "        conn,\n",
    "        df,\n",
    "        table_name=\"ACCIDENTAL_DRUG_RELATED_DEATHS_CLEAN\",\n",
    "        auto_create_table=True,\n",
    "    )\n",
    "    print(f\"Snowflake load (clean): success={success}, rows={rows}\")\n",
    "\n",
    "    # Load forecast table (if produced)\n",
    "    if forecasted_monthly_death is not None:\n",
    "        success_f, chunks_f, rows_f, _ = write_pandas(\n",
    "            conn,\n",
    "            forecasted_monthly_death,\n",
    "            table_name=\"ACCIDENTAL_DRUG_RELATED_DEATHS_FORECAST\",\n",
    "            auto_create_table=True,\n",
    "        )\n",
    "        print(f\"Snowflake load (forecast): success={success_f}, rows={rows_f}\")\n",
    "\n",
    "    conn.close()\n",
    "else:\n",
    "    if EXPORT_TO_SNOWFLAKE:\n",
    "        print(\"Snowflake env vars not set — skipping Snowflake export.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94426bc",
   "metadata": {},
   "source": [
    "## Next steps (if you want to extend this)\n",
    "- Add a `requirements.txt` and a small `Makefile` (or `uv` / `poetry`) so the notebook runs in one command.\n",
    "- Add data tests (e.g., `great_expectations` or simple `pytest` checks) to guard schema + null rates.\n",
    "- Add incremental loads (date-based) and a Snowflake task for scheduled refresh.\n",
    "- Include a screenshot / link to your Tableau dashboard in the repository README.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
